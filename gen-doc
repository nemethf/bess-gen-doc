#!/usr/bin/env python3

# Copyright (C) 2018 by Felicián Németh
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <http://www.gnu.org/licenses/>.

"""
Generate documentation for bess.


1. Clone or update bess repository,
2. Compile bess with container_build,
3. Extract documentation from protobuf messages,
4. Collect modules and their methods by running bessd in a container,
5. Find method definitions by running gdb,
6. Write a dummy module (globals.py).
"""

# Uninstall
#
# $ docker image rm gen-bess-doc-globals
# $ docker image rm nefelinetworks/bess_build:latest-git
# $ docker image rm nefelinetworks/bess_build
# $ docker image rm pseudomuto/protoc-gen-doc
#

import argparse
import collections
import functools
import gzip
import inspect
import json
import logging
import os
import re
import shutil
import subprocess
from pathlib import Path

import logging
logger = logging.getLogger('root')
FORMAT = "%(filename)s:%(lineno)s %(funcName)s %(message)s"
logging.basicConfig(format=FORMAT)
logger.setLevel(logging.ERROR)

wdir = Path(__file__).parent.resolve()
bess_dir = wdir / 'bess'
db = None

class FileWithLineNumber(object):
    def __init__(self, f):
        self.f = f
        self.line = 0

    def write(self, str):
        self.line += str.count("\n")
        self.f.write(str)


def check_call(cmd, **kw):
    "Like subprocess.check_call, but args can be Path objects as well."
    if type(cmd) == list:
        cmd = [ str(item) for item in cmd ]
    for k, v in kw.items():
        if isinstance(v, Path):
            kw[k] = str(v)
    return subprocess.check_call(cmd, **kw)

def update_bess():
    if not bess_dir.exists():
        cmd = ['git', 'clone', '-q', 'https://github.com/NetSys/bess.git']
        check_call(cmd, cwd=wdir)
    else:
        cmd = ['git', 'pull', '-q', 'origin', 'master']
        check_call(cmd, cwd=bess_dir)

def compile_bess():
    build_cmd = 'container_build.py'
    safe_version = wdir / 'safe_container_build.py'
    if not safe_version.exists():
        shutil.copyfile(str(bess_dir / build_cmd), str(safe_version))
    else:
        cmd = ['diff', '-q',
               str(safe_version.resolve()),
               str((bess_dir / build_cmd).resolve())]
        try:
            subprocess.check_output(cmd, cwd=str(wdir))
        except subprocess.CalledProcessError as e:
            print("%s" % e)
            print("There's a new version of %s" % build_cmd)
            print("If you think it's safe to run, run this command:")
            print(" cp '%s' '%s'" % (bess_dir / build_cmd, safe_version))
            exit(1)

    # We need a new image with git to generate a proper version
    # string.

    # We must keep the nefelinetworks prefix, because
    # container_build.py lets us customize only the tag_suffix.
    build_dir = wdir / 'bess-build'
    docker_build_image('nefelinetworks/bess_build:latest-git', build_dir)

    cmd = [build_cmd, 'bess']
    env = os.environ.copy()
    env['TAG_SUFFIX'] = '-git'
    env['DEBUG'] = '1'
    env['PATH'] += ':.'
    check_call(cmd, cwd=bess_dir, env=env)

def run_protoc_gen_doc():
    for f in (wdir/'out-protoc-gen-doc').glob('*.json'):
        f.unlink()

    run_protoc_gen_doc1()
    for i, plugin in enumerate(find_current_plugins()):
        run_protoc_gen_doc1(i, plugin)

def run_protoc_gen_doc1(id=None, plugin=None):
    image = 'pseudomuto/protoc-gen-doc'
    outdir = wdir/'out-protoc-gen-doc'
    if plugin:
        outfile = 'plugin-%s.json' % id
        indir = Path(plugin)/'protobuf'
    else:
        outfile = 'bess-protobuf.json'
        indir = bess_dir/'protobuf'
    outdir.mkdir(exist_ok=True)
    cmd = ['docker', 'run', '--rm',
           '-v', '%s:/out' % outdir.resolve(),
           '-v', '%s:/protos:ro' % indir.resolve(),
           image,
           # '--include_source_info',
           '--doc_opt=json,%s' % outfile]
    check_call(cmd, cwd=wdir)

def get_protobuf_dir(json_filename):
    m = re.search(r'plugin-(\d+).json$', str(json_filename))
    if m:
        idx = int(m.group(1)) + 1
    else:
        idx = 0
    dirs = [bess_dir] + find_current_plugins()
    return [Path(d)/'protobuf' for d in dirs][idx]

def iterate_proto_files():
    dirs = [bess_dir.resolve()] + [Path(d) for d in find_current_plugins()]
    for d in dirs:
        for f in (d/'protobuf').glob('*.proto'):
            yield f

def docker_build_image(image_name, work_dir):
    "Build the image if it does not exist."
    cmd = ['docker', 'images', '-q', image_name]
    if not subprocess.check_output(cmd):
        cmd = ['docker', 'build', '-t', image_name, '.']
        subprocess.run(cmd, cwd=str(work_dir))

def gen_globals_info():
    image = 'gen-bess-doc-globals'
    outdir = wdir / 'out-globals'
    src_dir = wdir / 'extract-globals-info'
    outdir.mkdir(exist_ok=True)

    docker_build_image(image, src_dir)

    # Run the image
    cmd = ['docker', 'run', '--rm',
           '-v', '%s:/out' % outdir.resolve(),
           '-v', '%s:/bess:ro' % bess_dir.resolve(),
           '-v', '%s:/bess-gen-doc:ro' % src_dir.resolve(),
           image]
    check_call(cmd, cwd=wdir)

def to_camel_case(snake_string):
    return snake_string.title().replace("_", "")

def get_gdb_info(gdb_proc, member_func):
    # Compile bess as: DEBUG=1 ./container_build.py
    #
    # (gdb) info line DRR::SetQuantumSize
    gdb_proc.stdin.write(('info line %s\n' % member_func).encode('utf-8'))
    gdb_proc.stdin.flush()
    response = ''
    while not response.endswith('.'):
        line = gdb_proc.stdout.readline().decode('utf-8').strip()
        response += line
    if response.endswith('not defined.'):
        return None
    m = re.match(r'.gdb. Line (\d+) of "([^"]+)"', response)
    if m:
        filename = m.group(2)
        return {'file': conv_file(filename, 'core'), 'line': int(m.group(1))}

    raise ValueError('Unknown gdb response: %s' % response)

@functools.lru_cache()
def find_exec_files():
    files = [bess_dir/'core'/'bessd']
    files.extend((bess_dir/'core'/'modules').glob('*.so'))
    return files

@functools.lru_cache(maxsize=None)
def find_gdb_proc(exec_file):
    proc = subprocess.Popen(['gdb', str(exec_file)],
                            stdin=subprocess.PIPE,
                            stdout=subprocess.PIPE,
                            stderr=subprocess.STDOUT)
    while True:
        line = proc.stdout.readline().decode('utf-8')
        # print(line.rstrip())
        if line.startswith("Reading symbols from "):
            break
    return proc

def find_def(class_name, func_name, msg_name):
    for exec_file in find_exec_files():
        proc = find_gdb_proc(exec_file)
        loc = find_def0(proc, class_name, func_name, msg_name)
        if loc and loc['file'] == 'core/module.cc' and func_name == '__init__':
            # find_def0 returned the base class implementation, which
            # means the module does not have a specialized
            # constructor.  So, return the location of ProcessBatch()
            # instead, which is more useful in this case.
            loc_pb = get_gdb_info(proc, "%s::ProcessBatch" % class_name)
            loc = loc_pb or loc
            logger.debug('ProcessBatch %s', loc)
        if loc:
            return loc
    logger.warn('Definition not found for %s, %s, %s',
                class_name, func_name, msg_name)
    return None

def find_def0(proc, class_name, func_name, msg_name):
    loc = None
    m = re.search('(\w+)Command(\w+)(Arg)', msg_name)
    if m:
        loc = get_gdb_info(proc, "%s::%s" % (m.group(1), m.group(2)))
    if m and not loc:
        loc = get_gdb_info(proc, "%s::Command%s" % (m.group(1), m.group(2)))
    if not loc:
        loc = get_gdb_info(proc, "%s::%s" % (class_name, to_camel_case(func_name)))
    if not loc:
        funcName = to_camel_case(func_name)
        loc = get_gdb_info(proc, "%s::Command%s" % (class_name, funcName))
    return loc

@functools.lru_cache()
def find_current_plugins():
    "Return list of existing plugins."
    # Adapted from from container_build.py
    result = []
    try:
        for line in (bess_dir/'core'/'extra.mk').open().readlines():
            match = re.match(r'PLUGINS \+= (.*)', line)
            if match:
                result.append(match.group(1))
    except (OSError, IOError):
        pass
    return result

@functools.lru_cache()
def find_sample_dirs():
    dirs = ['bessctl/conf/samples']
    for d in find_current_plugins():
        dirs.append(os.path.join(d, 'bessctl_conf'))
    return dirs

def find_usage_exampe(class_name):
    logger.debug('class_name=%s', class_name)
    # grep -Horn -m 1 '\bSink(' bessctl/conf/samples
    if not re.match(r'\w+', class_name):
        return []
    location = []
    for sample_dir in find_sample_dirs():
        cmd = ['grep', '-Hornm', '1', '\\b%s(' % class_name, str(sample_dir)]
        try:
            output = subprocess.check_output(cmd, cwd=str(bess_dir))
        except subprocess.CalledProcessError as e:
            if e.returncode == 1:
                # no matches found
                continue
            raise e
        for line in output.decode('utf-8').strip().split('\n'):
            parts = line.split(":")
            location.append({'file': conv_file(parts[0]),
                             'line': int(parts[1])})
    return sorted(location, key=lambda l: (l['file'], l['line']))

def set_protobuf_lineno(messages):
    "Modify each msg by adding the line number where msg was defined."
    # Ideally, we should modify pseudomuto/protoc-gen-doc instead of
    # this hack.
    loc = collections.defaultdict(dict)
    for fname in iterate_proto_files():
        with fname.open() as f:
            for lineno, line in enumerate(f, start=1):
                m = re.match(r'^\s*message\s+(\S+)', line)
                if not m:
                    continue
                loc[m.group(1)][conv_file(fname)] = lineno
    for m in messages.values():
        name = m['name']
        try:
            m['line'] = loc[name][m['file']]
        except KeyError:
            logger.debug('found no line number for %s', name)

def create_combined_db():
    msg = {}
    for fname in (wdir/'out-protoc-gen-doc').glob('*.json'):
        with fname.open() as f:
            protobuf = json.load(f)
        protobuf_dir = get_protobuf_dir(fname)
        for f in protobuf['files']:
            for m in f['messages']:
                m['file'] = conv_file(protobuf_dir/f['name'])
                msg[m['fullName']] = m
    set_protobuf_lineno(msg)
    msg_short = { m['name']: m for m in msg.values()}

    with (wdir / 'out-globals' / 'globals.json').open() as f:
        bess_globals = json.load(f)
    for obj in bess_globals['globals']:
        class_name = obj['name']
        cmd_arg = '%s%s' % (class_name, 'Arg')
        if cmd_arg not in msg_short:
            # MPLSPopArg -> MplsPopArg,
            # based on https://stackoverflow.com/a/1176023
            s1 = re.sub('(.)([A-Z][a-z]+)', r'\1_\2', cmd_arg)
            s2 = re.sub('([a-z0-9])([A-Z])', r'\1_\2', s1).lower()
            cmd_arg = to_camel_case(s2)
        cmd_name = '__init__'
        class_msg = msg_short.get(cmd_arg)
        obj['definition'] = find_def(class_name, cmd_name, cmd_arg)
        obj['examples'] = find_usage_exampe(class_name)
        obj['arg'] = cmd_arg
        for cmd in obj['cmds']:
            r_full = msg_short[cmd['arg']]['fullName']
            r_arg = re.sub(r'Arg$', 'Response', r_full)
            m = msg.get(r_arg)
            r_empty_full = 'bess.pb.EmptyResponse'
            if m and r_full.endswith('Arg') and r_arg != r_empty_full:
                cmd['return'] = r_arg
            elif cmd['cmd'].startswith('get_'):
                set_cmd = cmd['cmd'].replace('get_', 'set_')
                for scmd in obj['cmds']:
                    if scmd['cmd'] == set_cmd:
                        r_full = msg_short[scmd['arg']]['fullName']
                        m = msg.get(r_full)
                        if m and scmd['arg'] != 'EmptyArg':
                            cmd['return'] = r_full
            cmd['definition'] = find_def(class_name, cmd['cmd'], cmd['arg'])
            if cmd['arg'] == 'EmptyArg':
                # Sometimes, if a command has no arguments, there is a
                # corresponding protobuf message for documentation
                # purpuses only. (It is never used anywhere.)
                postfix = 'Command%sArg' % to_camel_case(cmd['cmd'])
                doc_msg = re.sub(r'Arg$', postfix, class_msg['fullName'])
                if msg.get(doc_msg):
                    cmd['doc_msg'] = doc_msg

    global db
    db = {
        'msg': msg,
        'globals': bess_globals['globals'],
        'bess-version': bess_globals['bess-version'],
    }
    # Write to a temp file, so next steps cloud run even if this step
    # does not run.
    db_write('-combined')
    return db

def indent(string, columns):
    ret = string.split('\n')
    ret = [' ' * columns + line if len(line)>0 else line for line in ret]
    ret = '\n'.join(ret)
    return ret

def conv_loc(location):
    filename = location['file']
    if not Path(filename).is_absolute():
        filename = '$BESS/' + filename
    return '%s:%s' % (filename, location['line'])

def conv_file(filename, rel_prefix=''):
    filename = str(filename)
    if os.path.isabs(filename):
        relative = bess_dir/rel_prefix
        abs_filename = os.path.abspath(filename)
        if relative.resolve().parts[:2] != Path(abs_filename).parts[:2]:
            return abs_filename
        else:
            return os.path.relpath(filename, str(bess_dir/rel_prefix))
    if rel_prefix:
        return os.path.join(rel_prefix, filename)
    return filename

def write_def(f, msg, msg_short, class_name, cmd):
    fields = []
    args = ["self"]
    m = msg_short.get(cmd['arg'])
    doc = []
    if m:
        doc.append(m['description'])
        fields += m['fields']
    if 'doc_msg' in cmd:
        doc.append(msg[cmd['doc_msg']]['description'])
    if cmd['cmd'] == '__init__':
        fields.append({'name': 'name', 'type': 'str', 'label': None})
    args_doc = []
    for field in fields:
        fname = field['name']
        try:
            fullType = field['fullType']
            ftype = db['msg'][fullType]['uniq_short_name']
        except KeyError:
            ftype = field['type']
        if field['label'] == 'repeated':
            ftype = 'List[%s]' % ftype
        if ARGS.typing:
            args.append('%s: %s = ...' % (fname, ftype))
        else:
            args.append('%s=None' % fname)
        if field.get('description'):
            args_doc.append(":param %s: %s" %
                            (field['name'], field['description']))

    doc.append('\n'.join(args_doc))
    m = msg.get(cmd.get('return'))
    if m:
        doc.append(":return: %s" % m['description'])
        # TODO: print "fields" here as well
    if ARGS.links and cmd.get('definition'):
        doc.append('Definition:\n  %s' % conv_loc(cmd['definition']))
    doc = [d for d in doc if d]
    if doc:
        doc = '\n"""\n%s\n"""' % '\n\n'.join(doc)
        doc = indent(doc, 4)
    else:
        doc = ''
    cmd['line'] = f.line + 1
    if ARGS.typing:
        ret = msg.get(cmd.get('return', ''), {}).get('name', '')
        ret = ret and " -> " + ret
        f.write("  def %s(\n    %s" % (cmd['cmd'], ",\n    ".join(args)))
        f.write("\n    )%s:%s\n    ...\n\n" % (ret, doc))
    else:
        f.write('  def %s(%s):%s\n    pass\n\n' % (
            cmd['cmd'], ", ".join(args), doc))

def camelCase(name):
    "Return CamelCase version of prtobuf field NAME."
    return name.replace('.', '')

def db_load():
    global db
    if db:
        return db

    with (wdir / 'out-combined' / 'globals.json').open() as f:
        db = json.load(f)
    return db

def db_check_and_prune():
    # bessctl uses short protobuf names, so those should be unique.
    # However, we use full names, because short return types and field
    # types are not necessarily unique.
    used = {}
    reported = {}
    def mark_used(used, name, full=False):
        if name is None:
            return
        if not full:
            try:
                name = msg_short[name]['fullName']
            except KeyError:
                logger.debug('no fullName for %s', name)
                return
        if name not in db['msg']:
            if name not in reported:
                logger.debug('unknown type: %s', name)
            reported[name] = True
            return
        if used.get(name):
            return
        if full:
            used[name] = 'arg'
        else:
            used[name] = used.get(name, 'top')
        for field in db['msg'][name].get("fields"):
            mark_used(used, field['fullType'], True)

    msg_short = {}
    for m in db['msg'].values():
        if m['name'] in msg_short:
            similar = msg_short[m['name']]
            logger.warning('Short name is not unique: %s, %s',
                           similar['fullName'], m['fullName'])
            similar['uniq_short_name'] = camelCase(similar['longName'])
            m['uniq_short_name'] = camelCase(m['longName'])
        msg_short[m['name']] = m

    for obj in db['globals']:
        methods = [obj] + obj['cmds']
        for method in methods:
            mark_used(used, method['arg'])
            mark_used(used, method.get('return'), True)
            mark_used(used, method.get("doc_msg"), True)

    to_delete = []
    for m in db['msg'].values():
        if m['fullName'] not in used:
            to_delete.append(m['fullName'])
        else:
            m['dependency'] = used[m['fullName']]
    for fullName in to_delete:
        logger.debug('delete unused msg: %s', fullName)
        del db['msg'][fullName]

def db_create_minimal(db):
    fields = [
        'defaultValue',
        'dependency',
        'description',
        'doc_msg',
        'extensions',
        'fields',
        'hasExtensions',
        'hasFields',
        'help',
        'label',
        'longName',
        'parent_cls',
        'uniq_short_name',
    ]
    def rm_fields(obj):
        if type(obj) == list:
            for i in obj:
                rm_fields(i)
            return
        if type(obj) == dict:
            for f in fields:
                obj.pop(f, None)
            for v in obj.values():
                rm_fields(v)
            return
        return

    def collect_files(obj, files):
        if type(obj) == list:
            for i in obj:
                collect_files(i, files)
            return files
        if type(obj) == dict:
            for v in obj.values():
                collect_files(v, files)
            if 'file' in obj:
                if type(obj['file']) == str and obj['file'] not in files:
                    files.append(obj['file'])
        return files

    def index_files(obj):
        if type(obj) == list:
            for i in obj:
                index_files(i)
            return
        if type(obj) == dict:
            for v in obj.values():
                index_files(v)
            if 'file' in obj:
                if type(obj['file']) == str:
                    obj['file'] = f_idx[obj['file']]
            elif 'line' in obj:
                obj['file'] = 0
            return
        return

    rm_fields(db)
    files = collect_files(db, [])
    f_idx = {fname: i for i, fname in enumerate(sorted(files), start=1)}
    f_idx['globals.py'] = 0
    index_files(db)
    db['files'] = {v: k for k, v in f_idx.items()}

    return db

def db_write(dir_suffix=''):
    outdir = 'out%s' % dir_suffix
    logger.debug('%s', outdir)
    (wdir / outdir).mkdir(exist_ok=True)
    db_file = wdir/outdir/'globals.json'
    db_minimal_file = wdir/outdir/'globals.min.json.gz'

    if not dir_suffix:
        msg_list = [m for _, m in sorted(db['msg'].items())]
        db['msg'] = msg_list

    with db_file.open('w') as f:
        json.dump(db, f, indent=2, sort_keys=True, separators=(',', ': '))

    if dir_suffix:
        return

    # Check the result with: zcat globals.min.json.gz|python3 -m json.tool
    db_min = db_create_minimal(db)
    with gzip.open(str(db_minimal_file), 'wt', encoding='utf8') as f:
        json.dump(db_min, f, sort_keys=True, separators=(',', ':'))

def set_parent_class(msg):
    cls = 'TypedDict, total=False'
    # pseudomuto/protoc-gen-doc doesn't support 'oneof':
    # https://github.com/pseudomuto/protoc-gen-doc/issues/333
    # https://github.com/pseudomuto/protoc-gen-doc/pull/336
    # https://github.com/pseudomuto/protoc-gen-doc/pull/362
    msg_with_oneOf = [
        'Field',                # util_msg.proto
        'FieldData',            # util_msg.proto
        'EncapField',
        'Attribute',
        'SplitArg',
    ]
    #if msg['longName'] in msg_with_oneOf:
    #    TODO
    msg['parent_cls'] = cls

def write_dummy_module():
    global db
    db = db_load()
    db_check_and_prune()

    simple_types = {
        # This doesn't seem to be true for python3
        # https://github.com/pseudomuto/protoc-gen-doc/blob/a40ad78161f7352e965d7698ea1c9cd7d2f8bbf2/resources/scalars.json#L164
        # 'bytes': 'str',
        'double': 'float',
        'fixed32': 'int',
        'fixed64': 'int',
        'int32': 'int',
        'int64': 'int',
        'sfixed32': 'int',
        'sfixed64': 'int',
        'sint32': 'int',
        'sint64': 'int',
        'string': 'str',
        'uint32': 'int',
        'uint64': 'int',
    }
    msg = db['msg']
    msg_short = { m.get('uniq_short_name', m['name']): m for m in msg.values()}
    (wdir / 'out').mkdir(exist_ok=True)
    with (wdir / 'out' / 'globals.py').open('w') as fobj:
        f = FileWithLineNumber(fobj)
        cfile = 'bess/protobuf/module_msg.proto'
        f.write("# This file is auto-genereated by bess-gen-doc.\n")
        f.write("# See https://github.com/nemethf/bess-gen-doc\n")
        f.write("#\n# It is based on %s, which "
                "has the following copyright.\n\n" % cfile)
        with (wdir / cfile).open() as cf:
            for line in cf:
                if not line.startswith('//'):
                    break
                f.write(line.replace('//', '#'))
        f.write("\n\n# Based on bess version: %s\n" % db['bess-version'])
        ignore = '# type: ignore'
        f.write("\n")
        f.write("from pybess.module import Module as BessModule %s\n" % ignore)
        f.write("from pybess.port import Port %s\n" % ignore)
        f.write("from pybess.bess import BESS %s\n\n" % ignore)
        f.write("bess = BESS()\n\n")

        if ARGS.typing:
            f.write('from typing import List, Any\n')
            f.write('from mypy_extensions import TypedDict\n\n')
            for stype, pytype in sorted(simple_types.items()):
                f.write("%s = %s\n" % (stype, pytype))
            f.write("\n")
            for m in sorted(msg_short.values(), key=lambda m: m['name']):
                if m['dependency'] == 'top':
                    continue
                set_parent_class(m)
                m['uniq_short_name'] = m.get('uniq_short_name', m['name'])
                f.write("class {uniq_short_name}({parent_cls}):\n".format(**m))
                for field in m['fields']:
                    ftype = field['type']
                    if field['name'] == 'class':
                        logger.error('%s has a field named "class"', m['name'])
                    if 'uniq_short_name' in msg_short.get(ftype, ''):
                        ftype = camelCase(msg[field['fullType']]['longName'])
                    if field['label'] == 'repeated':
                        ftype = 'List[%s]' % ftype
                    f.write("  {name}: {ftype}\n".format(ftype=ftype, **field))
                if not m['fields']:
                    f.write("  pass\n")
                f.write("\n")

        for info in sorted(db['globals'], key=lambda i: i['name']):
            class_name = info['name']
            cmd_arg = info['arg']
            doc = [info['help']]
            class_doc = msg_short.get(cmd_arg)
            if class_doc:
                doc.append(class_doc['description'])
            if ARGS.links and info['definition']:
                doc.append('Definition:\n  %s' % conv_loc(info['definition']))
            if ARGS.links and info['examples']:
               ex = [conv_loc(loc) for loc in info['examples']]
               doc.append('Example usage:\n  %s' % '\n  '.join(ex))

            parent = {'mclass': 'BessModule', 'driver': 'Port'}[info['type']]
            f.write('class %s(%s):\n' % (class_name, parent))
            info['line'] = f.line
            doc = [d for d in doc if d]
            if doc:
                doc = indent('\n\n'.join(doc), 2)
                f.write('  """\n%s\n  """\n\n' % doc)
            if class_doc:
                cmd = {
                    'arg': cmd_arg,
                    'cmd': '__init__',
                    'definition': info['definition'],
                }
                write_def(f, msg, msg_short, class_name, cmd)

            for cmd in info['cmds']:
                write_def(f, msg, msg_short, class_name, cmd)
            f.write('\n')
    db_write()

def main(args):
    steps = [
        update_bess,
        compile_bess,
        run_protoc_gen_doc,
        gen_globals_info,
        create_combined_db,
        write_dummy_module,
    ]
    for i, step in enumerate(steps, start=1):
        if i < args.start:
            continue
        logger.debug('Starting step %s: %s', i, step.__name__)
        step()


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.description = inspect.getdoc(inspect.getmodule(main))
    parser.add_argument('--start', '-s', type=int, default=1, metavar='S',
                        help="start at step S (while skipping previous steps)")
    parser.add_argument('--links', '-l', action='store_true',
                        help="add 'links' to docstrings")
    parser.add_argument('--typing', dest='typing', action='store_true',
                        help=("include type annotations in the .py file"
                              " (default)"))
    parser.add_argument('--no-typing', dest='typing', action='store_false',
                        help="don't include type annotations")
    parser.set_defaults(typing=True)
    parser.add_argument('--verbose', '-v', action='store_true')
    ARGS = parser.parse_args()
    if ARGS.verbose:
        logger.setLevel(logging.DEBUG)
    main(ARGS)
